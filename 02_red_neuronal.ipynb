{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal en PyTorch\n",
    "\n",
    "En este notebook veremos como crear una red neuronal usando PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero llamamos a los paquetes necesarios\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Ahora lo que necesitamos es un conjunto de datos (dataset). Afortunadamente el paquete **torchvision** provee un conjunto de datos de ejemplo. Utilizaremos MNIST, el cual contiene ejemplos de letras escritas a mano. El siguiente código lee el conjunto de datos y lo separa en un conjunto de entrenamiendo y uno de prueba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generaramos una transformación para normalizar el conjunto de datos\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                             ])\n",
    "# Descargamos el conjunto de datos de entrenamiento\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "# Cargamos el conjunto\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Descargamos y cargamos el conjunto de prueba\n",
    "testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenamos los datos para tener parejas de imagenes con su respectiva clase\n",
    "\n",
    "# Los datos se encuentran en trainloader asi que generamos un iterador para extraerlos uno por uno\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es recomendable verificar que estamos cargando bien el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAG55JREFUeJzt3XusbmV9J/DvT47AiOV4iUpMp+UyXAxWHcCCELkcI4NtVFSYYFIlRht1zFisTjRVnGNhGk3MeGMGTW1LKnGwgYhxStVRQBB01EOUIV64HhlSEZHhJqCiz/zxrq2n273PZb/v2Wvv5/18kjfPftdaz1q/s1js737ed12qtRYAoE+PGbsAAGD3EfQA0DFBDwAdE/QA0DFBDwAdE/QA0DFBDwAdE/QA0DFBDwAdE/QA0DFBDwAdE/QA0DFBDwAdE/QA0DFBDwAdE/QA0LENYxewO1TVbUn2TbJ15FIAYKX2T3J/a+2AaVbSZdBnEvJPGl4AMLdG/ei+qn63qv62qv65qn5WVVur6oNV9cQpV711FvUBwMi2TruC0Ub0VXVQkmuTPDXJZ5J8L8kfJvmzJKdU1XGttZ+MVR8A9GDMEf1/zyTk39xaO7W19o7W2qYkH0hyaJL/MmJtANCFaq2t/karDkxySyYfSRzUWvvVNvN+J8kPk1SSp7bWfrqC9W9JcsRsqgWA0VzXWjtymhWMNaLfNLRf2Dbkk6S19kCSa5I8Lskxq10YAPRkrO/oDx3aG5eZf1OSk5MckuRLy61kGLkv5bCVlwYA/RhrRL9xaO9bZv7C9CesQi0A0K21eh19De12TyBY7nsL39EDwMRYI/qFEfvGZebvu2g5AGAFxgr67w/tIcvMP3hol/sOHwDYCWMF/RVDe3JV/YsahsvrjkvycJKvrXZhANCTUYK+tXZLki9kcsP+Ny2a/Z4k+yT5+5VcQw8A/MaYJ+P9h0xugfvhqnpBku8mOTrJSZl8ZP/OEWsDgC6MdgvcYVR/VJILMgn4tyY5KMmHkzzPfe4BYHqjXl7XWvu/SV4zZg0A0LNRH1MLAOxegh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjG8YuAFi5U045Zar+l1122Ywq2XU33XTTVP2POuqoFfd94IEHpto2rCdG9ADQMUEPAB0T9ADQMUEPAB0T9ADQMUEPAB0T9ADQMUEPAB0T9ADQMUEPAB0T9ADQMUEPAB0T9ADQMUEPAB3zmFoY2QEHHLDivueff/5U226tTdV/GgcffPBU/V/60peuuO+FF1441bZhPTGiB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4CO1ZjPo95dqmpLkiPGrgN2xo033rjivgcddNAMK1ldVTVV/2uuuWbFfZ///OdPtW1YRde11o6cZgWjjeiramtVtWVed45VFwD0ZMPI278vyQeXmP7gahcCAD0aO+jvba1tHrkGAOiWk/EAoGNjj+j3qqo/SfJ7SX6a5PokV7XWfjluWQDQh7GDfr8kn1g07baqek1r7cs76jycXb+Uw6auDAA6MOZH93+X5AWZhP0+Sf4gyceS7J/kn6rq2eOVBgB9GG1E31p7z6JJNyR5Q1U9mOStSTYnedkO1rHktYWuoweAibV4Mt5Hh/b4UasAgA6sxaC/a2j3GbUKAOjAWgz65w3traNWAQAdGCXoq+rwqnrSEtN/P8l5w9sLV7cqAOjPWCfjnZ7kHVV1RZLbkjyQ5KAkf5xk7ySXJXn/SLUBQDfGCvorkhya5N9m8lH9PknuTfKVTK6r/0Tr8bF6ALDKRgn64WY4O7whDqwHGzdunKr/fvvtN6NK5svHPvaxsUuAdWEtnowHAMyIoAeAjgl6AOiYoAeAjgl6AOiYoAeAjgl6AOiYoAeAjgl6AOiYoAeAjgl6AOiYoAeAjgl6AOiYoAeAjgl6AOjYKM+jh568/e1vn6r/PvvsM6NKVtc3vvGNqfp/8YtfnKr/Zz7zman6w7wwogeAjgl6AOiYoAeAjgl6AOiYoAeAjgl6AOiYoAeAjgl6AOiYoAeAjgl6AOiYoAeAjgl6AOiYoAeAjgl6AOiYx9TClKpq1P7TuP/++1fc95hjjplhJcDuYkQPAB0T9ADQMUEPAB0T9ADQMUEPAB0T9ADQMUEPAB0T9ADQMUEPAB0T9ADQMUEPAB0T9ADQMUEPAB0T9ADQMUEPAB3zPHqY0nOf+9yp+rfWZlTJrvurv/qr0bYNrA4jegDomKAHgI4JegDomKAHgI4JegDomKAHgI4JegDomKAHgI4JegDomKAHgI4JegDomKAHgI4JegDomKAHgI55TC0kOfjgg1fcd9OmTVNte5rH1P785z+fatu33HLLVP2BtW8mI/qqOq2qPlJVV1fV/VXVqurCHfQ5tqouq6p7quqhqrq+qs6qqj1mURMAMLsR/buSPDvJg0nuSHLY9hauqpcmuSTJI0k+leSeJC9O8oEkxyU5fUZ1AcBcm9V39G9JckiSfZO8cXsLVtW+Sf46yS+TnNhae21r7T8leU6SryY5rarOmFFdADDXZhL0rbUrWms3tZ37svG0JE9JclFr7ZvbrOORTD4ZSHbwxwIAsHPGOOt+4cylzy0x76okDyU5tqr2Wr2SAKBPYwT9oUN74+IZrbVHk9yWybkDB65mUQDQozEur9s4tPctM39h+hN2tKKq2rLMrO2eDAgA82It3jCnhnblFxcDAEnGGdEvjNg3LjN/30XLLau1duRS04eR/hG7XhoA9GWMEf33h/aQxTOqakOSA5I8muTW1SwKAHo0RtBfPrSnLDHv+CSPS3Jta+1nq1cSAPRpjKC/OMndSc6oqqMWJlbV3knOHd6eP0JdANCdmXxHX1WnJjl1eLvf0D6vqi4Yfr67tfa2JGmt3V9Vf5pJ4F9ZVRdlcgvcl2Ry6d3FmdwWFwCY0qxOxntOkjMXTTswv7kW/gdJ3rYwo7V2aVWdkOSdSV6RZO8kNyf58yQf3sk77AEAOzCToG+tbU6yeRf7XJPkj2axfQBgaZ5HD0k2b948dgkrcscdd0zV/5JLLplRJcBatRZvmAMAzIigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COeUwtMIpPfvKTU/Vvra247y233DLVtt/97ndP1R9WkxE9AHRM0ANAxwQ9AHRM0ANAxwQ9AHRM0ANAxwQ9AHRM0ANAxwQ9AHRM0ANAxwQ9AHRM0ANAxwQ9AHRM0ANAxwQ9AHSspnmm81pVVVuSHDF2HcyHaf8fGvP/wTe84Q0r7nviiSdOte1XvvKVU/Ufc7898sgjK+57yimnTLXtq666aqr+rDvXtdaOnGYFRvQA0DFBDwAdE/QA0DFBDwAdE/QA0DFBDwAdE/QA0DFBDwAdE/QA0DFBDwAdE/QA0DFBDwAdE/QA0DFBDwAd2zB2AbDerefH1E5j7H/3mPttr732WnHfk08+eapte0wtu8qIHgA6JugBoGOCHgA6JugBoGOCHgA6JugBoGOCHgA6JugBoGOCHgA6JugBoGOCHgA6JugBoGOCHgA6JugBoGOCHgA65nn0MMfOPffcFfed5pns8+yggw4auwTmzExG9FV1WlV9pKqurqr7q6pV1YXLLLv/MH+510WzqAkAmN2I/l1Jnp3kwSR3JDlsJ/p8O8mlS0y/YUY1AcDcm1XQvyWTgL85yQlJrtiJPt9qrW2e0fYBgCXMJOhba78O9qqaxSoBgBkY82S8p1fV65M8OclPkny1tXb9iPUAQHfGDPoXDq9fq6ork5zZWrt9Z1ZQVVuWmbUz5wgAQPfGuI7+oSTnJDkyyROH18L3+icm+VJV7TNCXQDQnVUf0bfW7kry7kWTr6qqk5N8JcnRSV6X5EM7sa4jl5o+jPSPmLJUAFj31syd8Vprjyb5+PD2+DFrAYBerJmgH/x4aH10DwAzsNaC/pihvXXUKgCgE6se9FV1dFXtucT0TZnceCdJlrx9LgCwa2ZyMl5VnZrk1OHtfkP7vKq6YPj57tba24af35fk8OFSujuGac9Ksmn4+ezW2rWzqAsA5t2szrp/TpIzF007cHglyQ+SLAT9J5K8LMlzk7woyWOT/CjJPyQ5r7V29YxqAoC5N6tb4G5Osnknl/2bJH8zi+0CANtXrbWxa5g519Gzmr74xS9O1f+kk06aUSXry7TPxejxd9fO2GOPPcYugdV13XL3jNlZa+2sewBghgQ9AHRM0ANAxwQ9AHRM0ANAxwQ9AHRM0ANAxwQ9AHRM0ANAxwQ9AHRM0ANAxwQ9AHRM0ANAxwQ9AHRsJs+jh3n29a9/far+8/qYWmB1GNEDQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMc8jx6mNO3z6H/xi1+suO+ee+451bbHVFVjlwBzwYgeADom6AGgY4IeADom6AGgY4IeADom6AGgY4IeADom6AGgY4IeADom6AGgY4IeADom6AGgY4IeADom6AGgYx5TC1O69NJLp+r/yCOPrLjvYx/72Km2vZ611sYuYRTHHXfcivtec801M6yE9cKIHgA6JugBoGOCHgA6JugBoGOCHgA6JugBoGOCHgA6JugBoGOCHgA6JugBoGOCHgA6JugBoGOCHgA6JugBoGOCHgA65nn0MLJzzz13xX3POeecqba95557TtWfXXfvvfdO1f/OO++cUSXMi6lH9FX15Kp6XVV9uqpurqqHq+q+qvpKVb22qpbcRlUdW1WXVdU9VfVQVV1fVWdV1R7T1gQATMxiRH96kvOT/DDJFUluT/K0JC9P8vEkL6qq01trbaFDVb00ySVJHknyqST3JHlxkg8kOW5YJwAwpVkE/Y1JXpLkH1trv1qYWFV/keTrSV6RSehfMkzfN8lfJ/llkhNba98cpp+d5PIkp1XVGa21i2ZQGwDMtak/um+tXd5a++y2IT9MvzPJR4e3J24z67QkT0ly0ULID8s/kuRdw9s3TlsXALD7z7r/xdA+us20TUP7uSWWvyrJQ0mOraq9dmdhADAPdttZ91W1Icmrh7fbhvqhQ3vj4j6ttUer6rYkhyc5MMl3d7CNLcvMOmzXqgWAPu3OEf17kzwzyWWttc9vM33j0N63TL+F6U/YXYUBwLzYLSP6qnpzkrcm+V6SV+1q96Ft210qSWvtyGW2vyXJEbu4XQDozsxH9FX1piQfSvKdJCe11u5ZtMjCiH1jlrbvouUAgBWaadBX1VlJzktyQyYhv9QtnL4/tIcs0X9DkgMyOXnv1lnWBgDzaGZBX1Vvz+SGN9/KJOTvWmbRy4f2lCXmHZ/kcUmuba39bFa1AcC8mknQDze7eW+SLUle0Fq7ezuLX5zk7iRnVNVR26xj7yQLN/0+fxZ1AcC8m/pkvKo6M8lfZnKnu6uTvLmqFi+2tbV2QZK01u6vqj/NJPCvrKqLMrkF7ksyufTu4kxuiwsATGkWZ90fMLR7JDlrmWW+nOSChTettUur6oQk78zkFrl7J7k5yZ8n+fC298UHAFZu6qBvrW1OsnkF/a5J8kfTbh/Wu/e///2jbfvss89ecd/HP/7xM6xkfXn44YdX3Pd973vfVNu+5ZZbpurP/Nndt8AFAEYk6AGgY4IeADom6AGgY4IeADom6AGgY4IeADom6AGgY4IeADom6AGgY4IeADom6AGgY4IeADom6AGgY4IeADpWrbWxa5i5qtqS5Iix64C1buPGjSvu+7WvfW2qbR966KFT9Z/md9e999471bZPOOGEFfe94YYbpto2c+e61tqR06zAiB4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjgh4AOiboAaBjG8YuABjPfffdt+K+z3jGM2ZYCbC7GNEDQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMcEPQB0TNADQMemDvqqenJVva6qPl1VN1fVw1V1X1V9papeW1WPWbT8/lXVtvO6aNqaAICJDTNYx+lJzk/ywyRXJLk9ydOSvDzJx5O8qKpOb621Rf2+neTSJdZ3wwxqAgAym6C/MclLkvxja+1XCxOr6i+SfD3JKzIJ/UsW9ftWa23zDLYPACxj6o/uW2uXt9Y+u23ID9PvTPLR4e2J024HANh1sxjRb88vhvbRJeY9vapen+TJSX6S5Kuttet3cz0AMFd2W9BX1YYkrx7efm6JRV44vLbtc2WSM1trt++uugBgnuzOEf17kzwzyWWttc9vM/2hJOdkciLercO0ZyXZnOSkJF+qque01n66ow1U1ZZlZh220qIBoCf12yfDz2ClVW9O8qEk30tyXGvtnp3osyHJV5IcneSs1tqHdqLP9oL+cTtfMQCsSde11o6cZgUzH9FX1ZsyCfnvJHnBzoR8krTWHq2qj2cS9McP69hRnyX/8cMfAEfsdNEA0KmZ3hmvqs5Kcl4m18KfNJx5vyt+PLT7zLIuAJhXMwv6qnp7kg8k+VYmIX/XClZzzNDeut2lAICdMpOgr6qzMzn5bksmH9ffvZ1lj66qPZeYvinJW4a3F86iLgCYd1N/R19VZyb5yyS/THJ1kjdX1eLFtrbWLhh+fl+Sw4dL6e4Ypj0ryabh57Nba9dOWxcAMJuT8Q4Y2j2SnLXMMl9OcsHw8yeSvCzJc5O8KMljk/woyT8kOa+1dvUMagIAspsurxubs+4B6MTUl9d5Hj0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHBD0AdEzQA0DHeg36/ccuAABmYP9pV7BhBkWsRfcP7dZl5h82tN/b/aV0wz5bGfttZey3XWefrcxa3m/75zd5tmLVWpu+lHWmqrYkSWvtyLFrWS/ss5Wx31bGftt19tnKzMN+6/WjewAggh4AuiboAaBjgh4AOiboAaBjc3nWPQDMCyN6AOiYoAeAjgl6AOiYoAeAjgl6AOiYoAeAjgl6AOjYXAV9Vf1uVf1tVf1zVf2sqrZW1Qer6olj17ZWDfuoLfO6c+z6xlJVp1XVR6rq6qq6f9gfF+6gz7FVdVlV3VNVD1XV9VV1VlXtsVp1j21X9ltV7b+dY69V1UWrXf8YqurJVfW6qvp0Vd1cVQ9X1X1V9ZWqem1VLfl7fN6Pt13dbz0fb70+j/63VNVBSa5N8tQkn8nk2cN/mOTPkpxSVce11n4yYolr2X1JPrjE9AdXu5A15F1Jnp3JPrgjv3mm9ZKq6qVJLknySJJPJbknyYuTfCDJcUlO353FriG7tN8G305y6RLTb5hhXWvZ6UnOT/LDJFckuT3J05K8PMnHk7yoqk5v29z9zPGWZAX7bdDf8dZam4tXks8naUn+46Lp/3WY/tGxa1yLryRbk2wdu4619kpyUpKDk1SSE4dj6MJllt03yV1JfpbkqG2m753JH58tyRlj/5vW4H7bf5h/wdh1j7zPNmUS0o9ZNH2/TMKrJXnFNtMdbyvbb90eb3Px0X1VHZjk5ExC678tmv2fk/w0yauqap9VLo11qrV2RWvtpjb8htiB05I8JclFrbVvbrOORzIZ4SbJG3dDmWvOLu43krTWLm+tfba19qtF0+9M8tHh7YnbzHK8ZUX7rVvz8tH9pqH9whL/0R+oqmsy+UPgmCRfWu3i1oG9qupPkvxeJn8UXZ/kqtbaL8cta91YOP4+t8S8q5I8lOTYqtqrtfaz1Str3Xh6Vb0+yZOT/CTJV1tr149c01rxi6F9dJtpjrcdW2q/LejueJuXoD90aG9cZv5NmQT9IRH0S9kvyScWTbutql7TWvvyGAWtM8sef621R6vqtiSHJzkwyXdXs7B14oXD69eq6sokZ7bWbh+lojWgqjYkefXwdttQd7xtx3b224Lujre5+Og+ycahvW+Z+QvTn7AKtaw3f5fkBZmE/T5J/iDJxzL5PuufqurZ45W2bjj+VuahJOckOTLJE4fXCZmcWHViki/N+ddt703yzCSXtdY+v810x9v2Lbffuj3e5iXod6SG1veGi7TW3jN81/Wj1tpDrbUbWmtvyOQkxn+VZPO4FXbB8beE1tpdrbV3t9aua63dO7yuyuTTt/+d5N8ked24VY6jqt6c5K2ZXD30ql3tPrRzd7xtb7/1fLzNS9Av/AW7cZn5+y5ajh1bOJnl+FGrWB8cfzPUWns0k8ujkjk8/qrqTUk+lOQ7SU5qrd2zaBHH2xJ2Yr8tqYfjbV6C/vtDe8gy8w8e2uW+w+e33TW06/KjrFW27PE3fF94QCYnBd26mkWtcz8e2rk6/qrqrCTnZXJN90nDGeSLOd4W2cn9tj3r+nibl6C/YmhPXuJuSL+TyQ0kHk7ytdUubB173tDOzS+LKVw+tKcsMe/4JI9Lcu0cnwG9EscM7dwcf1X19kxuePOtTMLqrmUWdbxtYxf22/as6+NtLoK+tXZLki9kcgLZmxbNfk8mf6X9fWvtp6tc2ppWVYdX1ZOWmP77mfx1nCTbve0rSZKLk9yd5IyqOmphYlXtneTc4e35YxS2llXV0VW15xLTNyV5y/B2Lo6/qjo7k5PItiR5QWvt7u0s7ngb7Mp+6/l4q3m5b8USt8D9bpKjM7lT141Jjm1ugfsvVNXmJO/I5BOR25I8kOSgJH+cyV22Lkvystbaz8eqcSxVdWqSU4e3+yX5d5n8tX/1MO3u1trbFi1/cSa3JL0ok1uSviSTS6EuTvLv5+EmMruy34ZLmg5PcmUmt8tNkmflN9eJn91aWwiublXVmUkuSPLLJB/J0t+tb22tXbBNn7k/3nZ1v3V9vI19a77VfCX515lcLvbDJD9P8oNMTs540ti1rcVXJpeW/I9MzlC9N5ObTPw4yf/K5DrUGrvGEffN5kzOWl7utXWJPsdl8sfR/8vkq6L/k8lIYY+x/z1rcb8leW2S/5nJHS0fzOSWrrdncu/254/9b1lD+6wludLxNt1+6/l4m5sRPQDMo7n4jh4A5pWgB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6JigB4COCXoA6Nj/B73WlcDs4H2CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9451abca90>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');\n",
    "images[1].size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de la red neuronal\n",
    "\n",
    "Ahora pasaremos a la creación de la red neuronal, como ejemplo utilizaremos una red completamente conectada (fully connected) para clasificar las imagenes de MNIST. Como entrada tendremos 784 nodos = 28 * 28, en seguida tendremos una capa oculta de 128 nodos, con una función de activación tipo RELU, despúes tendremos una segunda capa oculta con 64 nodos y función de activación RELU, en seguida tendremos 10 nodos de salida los cuales pasan por una función softmax que convierte los valores a probabilidades. Finalmente calculamos la pérdida (loss) con la función de entropía cruzada. \n",
    "\n",
    "<img src=\"archivos/net.png\">\n",
    "\n",
    "El modulo que contiene las herramientas para crear la RN es **pytorch.nn**. La red neuronal en sí se crea como una clase que hereda la estructura de **pytorch.nn.Module**. Cada una de las capas de la red se define de forma independiente. e.g. Para crear una capa con 784 entradas y 128 nodos utilizamos *nn.Linear(784, 128)*\n",
    "\n",
    "La red implementa la función *forward* que realiza el paso frontal (fowdward pass). Esta función miembro recibe un tensor como emtrada y calcula la salida de la red. \n",
    "\n",
    "Varias funciones de activación se encuntran en el módulo *nn.functional*. Dicho módulo usualmente se importa como *F*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Definir las capas. Cada una con 128, 64 y 10 unidades respectivamente\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # Capa de salida con 10 units (una para cada dígito)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Pase frontal de la red, regresamos las probabilidades '''\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializamos pesos y sesgos\n",
    "\n",
    "Cuando creas las capas se crean también los tensores correspondientes a los pesos y sesgos. Éstos son inicializados por ti, aunque pudes modificarlos usando funciones extra. Para observar sus valores puedes llamar a *model.fc1.weight* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0084,  0.0259,  0.0028,  ..., -0.0354,  0.0007,  0.0308],\n",
      "        [-0.0131,  0.0080,  0.0204,  ...,  0.0182,  0.0352, -0.0337],\n",
      "        [ 0.0080, -0.0337,  0.0068,  ..., -0.0243,  0.0047,  0.0168],\n",
      "        ...,\n",
      "        [ 0.0080,  0.0205, -0.0325,  ...,  0.0083, -0.0127, -0.0346],\n",
      "        [ 0.0092, -0.0274,  0.0241,  ...,  0.0019,  0.0040,  0.0191],\n",
      "        [-0.0049, -0.0224, -0.0022,  ..., -0.0105, -0.0306, -0.0065]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0277, -0.0282,  0.0277, -0.0131, -0.0071, -0.0162,  0.0187,  0.0201,\n",
      "         0.0087,  0.0057,  0.0175,  0.0105,  0.0197,  0.0245,  0.0125,  0.0306,\n",
      "        -0.0111, -0.0175,  0.0124,  0.0273, -0.0240,  0.0027, -0.0312, -0.0168,\n",
      "         0.0065, -0.0171,  0.0315,  0.0249,  0.0110, -0.0061,  0.0255, -0.0148,\n",
      "         0.0096, -0.0271, -0.0085, -0.0173, -0.0307,  0.0106, -0.0146,  0.0057,\n",
      "         0.0072, -0.0344,  0.0136, -0.0345,  0.0323, -0.0125,  0.0335, -0.0075,\n",
      "         0.0188,  0.0163,  0.0050,  0.0041,  0.0111, -0.0163,  0.0284, -0.0201,\n",
      "         0.0355, -0.0339, -0.0175,  0.0277,  0.0159,  0.0115,  0.0106, -0.0112,\n",
      "         0.0005, -0.0290, -0.0061, -0.0171,  0.0305,  0.0315,  0.0328,  0.0136,\n",
      "        -0.0286, -0.0089, -0.0067, -0.0155, -0.0316,  0.0217, -0.0062,  0.0166,\n",
      "         0.0003, -0.0299, -0.0054, -0.0338,  0.0148,  0.0032, -0.0006, -0.0185,\n",
      "        -0.0051, -0.0021,  0.0305,  0.0189,  0.0259, -0.0144, -0.0232,  0.0072,\n",
      "        -0.0255, -0.0320,  0.0133,  0.0087,  0.0073,  0.0271, -0.0028,  0.0245,\n",
      "        -0.0131, -0.0302, -0.0211, -0.0289, -0.0192, -0.0010, -0.0009,  0.0234,\n",
      "         0.0216,  0.0196,  0.0326, -0.0031,  0.0353,  0.0249,  0.0246,  0.0165,\n",
      "         0.0002, -0.0292,  0.0258, -0.0135,  0.0253,  0.0226, -0.0303,  0.0097],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que deseamos inicializar los pesos con algunos valores personalizados. Dado que los pesos y sesgos en sí son variables de autograd (optimizador) necesitamos convertirlos a tensores para poder modificarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colocamos ceros\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0155,  0.0073, -0.0050,  ..., -0.0211, -0.0098, -0.0031],\n",
       "        [ 0.0099, -0.0055,  0.0173,  ..., -0.0099,  0.0161, -0.0035],\n",
       "        [ 0.0048, -0.0058,  0.0151,  ...,  0.0083, -0.0232,  0.0059],\n",
       "        ...,\n",
       "        [ 0.0238, -0.0010,  0.0008,  ..., -0.0048,  0.0017,  0.0200],\n",
       "        [-0.0027, -0.0163, -0.0073,  ..., -0.0086, -0.0150, -0.0104],\n",
       "        [ 0.0109, -0.0031, -0.0099,  ..., -0.0010,  0.0022,  0.0102]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# muestreamos de una distribución normal con media cero y desv. estandar = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pase frontal\n",
    "\n",
    "Hasta el momento la red no está entrenada y solo tenemos los pesos aleatorios. Hagamos un pase frontal para ver que pasa. Primero debemos convertir la imagen a un tensor y pasarla a través de la red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtengamos el siguiente valor \n",
    "#dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Reestructuremos la imagen a un vector de una dimensión, hay quie le llama a esta operación \"aplanado\".\n",
    "# La nueva forma será (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# alternativa: images.resize_(images.shape[0], 1, 784) to not automatically get batch size\n",
    "\n",
    "# Pase frontal de la red\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0884, 0.1001, 0.1008, 0.1114, 0.0861, 0.1096, 0.1102, 0.0924, 0.0930,\n",
       "         0.1079]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps\n",
    "\n",
    "#img = images[img_idx]\n",
    "#helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy probablemente ninguna de las clases tiene una probabilidad grande con respecto de las otras, esto se debe a que todavía no hemos entrenado la red. En el siguiente ejercicio entrenaremos la red.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
