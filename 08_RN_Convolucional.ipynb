{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal Convolucional\n",
    "\n",
    "Una red neuronal convolucional es un tipo de de red profunda en la que se comparten los pesos. Una de las redes colvulucionales más famosas es LeNet5 de Yan LeCun. En este notebook implementaremos la red neuronal convolucional Lenet5 con PyTorch.\n",
    "\n",
    "<img src=\"archivos/lenet.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos paquetes\n",
    "# Cargamos paquetes necesarios\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#helper was developed by Udacity under MIT license\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo usaremos el conjunto de datos FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Definimos una transformación de los datos\n",
    "transform = transforms.Compose([transforms.Resize(32),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Descargamos el conjunto de entrenamiento y cargamos mediante un dataLoader\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size, shuffle=True)\n",
    "\n",
    "# Descargamos el conjunto de validación\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size, shuffle=True)\n",
    "\n",
    "# Print out some stats about the training and test data\n",
    "print('Train data, number of images: ', len(trainset))\n",
    "print('Test data, number of images: ', len(testset))\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "display_size = 10\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(display_size):\n",
    "    ax = fig.add_subplot(2, display_size, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la RNC\n",
    "Si estudiamos la arquitectura obsevaremos que esta compuesta por 2 capas convolucionales, despúes de cada convolución se realiza un max pooling. Al final de la red se agrega una red completamente conectada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, n_clases):\n",
    "        '''\n",
    "        Construimos la estructura de LeNet5\n",
    "        \n",
    "        '''\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        # De acuerdo al artículo de LeCun La primera capa está compuesta por 6 kernels de 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # 1 canal de entrada 6 feature maps de salida, kernels de 5x5\n",
    "        \n",
    "        # Después tenemos una capa maxpooling\n",
    "        # kernel_size=2, stride=2\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Agregamos otra capa convolucional con 16 kernels de 5 x 5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Maxpooling\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Capas totalmente conectadas\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, n_clases)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Definimos el pase frontal (forward pass)\n",
    "        '''\n",
    "        # Agregamos los ReLUs\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        #print(x.size())\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        # prep for linear layer by flattening the feature maps into feature vectors\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # capas lineales\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y validación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "model = LeNet5(10)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# información de la red\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos una función de evaluación\n",
    "def validation(model, testloader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in testloader:\n",
    "\n",
    "        images.resize_(images.shape[0], 784)\n",
    "\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Calculate accuracy before training\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate through test dataset\n",
    "for images, labels in testloader:\n",
    "    \n",
    "    # warp input images in a Variable wrapper\n",
    "    images = Variable(images)\n",
    "\n",
    "    # forward pass to get outputs\n",
    "    # the outputs are a series of class scores\n",
    "    outputs = model(images)\n",
    "\n",
    "    # get the predicted class from the maximum value in the output-list of class scores\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # count up total number of correct labels\n",
    "    # for which the predicted and true labels are equal\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "# print it out!\n",
    "print('Accuracy before training: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "#hiperparámetro\n",
    "\n",
    "epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 40\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    # Cambiamos a modo entrenamiento\n",
    "    #model.train()\n",
    "    \n",
    "    #for batch_i, data in enumerate(train_loader):\n",
    "            # get the input images and their corresponding labels\n",
    "    #        inputs, labels = data\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        steps += 1\n",
    "        \n",
    "        # Aplanar imágenes a un vector de 784 elementos\n",
    "        # images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # wrap them in a torch Variable\n",
    "        images, labels = Variable(images), Variable(labels)       \n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Optimización\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Cambiamos a modo de evaluación\n",
    "            #model.eval()\n",
    "            \n",
    "            # Apagamos los gradientes, reduce memoria y cálculos\n",
    "            #with torch.no_grad():\n",
    "            #    test_loss, accuracy = validation(model, testloader, criterion)\n",
    "                \n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                  \"Training Loss: {:.3f}.. \".format(running_loss/print_every))\n",
    "            #      \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "            #      \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            # Make sure training is back on\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "display_size = 10\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "# get predictions\n",
    "preds = np.squeeze(model(Variable(images, volatile=True)).data.max(1, keepdim=True)[1].numpy())\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(display_size):\n",
    "    ax = fig.add_subplot(2, display_size, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
