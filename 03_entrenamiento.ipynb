{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de una red neuronal\n",
    "\n",
    "Al entrenar una red buscamos que ésta tenga un comportamiendo presumiblemente adecuado y observable en un conjunto de datos. Es decír a partir del conjunto de ejemplos esperamos que la red aprenda una función $f$ tal que la salida de la red imite el patrón en los datos. En nuestro caso, al utilizar el conjunto de entrenamiento MNIST esperamos que la función aprendida provea como salida el número al cual corresponde la imagen de entrada.\n",
    "\n",
    "<img src=\"archivos/function_approx.png\">\n",
    "\n",
    "Imagen tomada de [1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retropropagación y Autograd\n",
    "\n",
    "Pytorch provee el módulo *autograd* para calcular los gradientes, y si, !nos evita estar calculando las derivadas! Esto lo realiza a partir de mantener en la vista todas las operaciones que se ejecutan sobre los tensores.\n",
    "\n",
    "Si deseas asegurarte que autograd siga a un tensor especificamos *requires_grad*. Esto se puede hacer en la creación o en cualquier momento. \n",
    "\n",
    "Veamos el siguiente código de ejemplo:\n",
    "\n",
    "```python\n",
    "# especificamos que la variable x es seguida por autograd\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    "# si en algún momento desamos que temporalmente se deje de seguir el tensor usamos\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    "# establecemos de nuevo el seguimiento\n",
    ">>> y.requires_grad\n",
    "```\n",
    "\n",
    "Si queremos eliminar autograd de todos los tensores usamos `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "\n",
    "Ahora bien, para calcular los gradientes simplemente usamos el método *backward()*. Por ejemplo para un tensor *cualquiera* hacemos *z.backward()*\n",
    "\n",
    "Veamos a continuación el uso del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos paquetes\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3605,  0.6653, -0.9058, -1.6002], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# especificamos que el tensor x es seguido por autograd \n",
    "x = torch.randn(4, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1299, 0.4426, 0.8205, 2.5606], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# generamos un nuevo tensor a partir de x\n",
    "# por ejemplo elevemos al cuadrado la variable x\n",
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x0000019E2FAB96D0>\n"
     ]
    }
   ],
   "source": [
    "## con grad_fn observamos la operación que generó y, es decir una operación potencia (pow)\n",
    "print(y.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9884, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# De esta forma es posible saber las operaciónes que generan cada tensor, y por tanto, es posible calcular el gradiente.\n",
    "# Hagamos ahora una operación de media\n",
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# hasta este momento los gradientes son cero\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular los gradientes es necesario llamar al método *.backward* sobre la variable. Supongamos sobre *z*. Esto calcula el gradiente de z con respecto de x.\n",
    "\n",
    "El gradiente analítico de las operaciónes que hicimos es:\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$\n",
    "\n",
    "Ahora comprobemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1802,  0.3326, -0.4529, -0.8001])\n",
      "tensor([-0.1802,  0.3326, -0.4529, -0.8001], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de datos y red neuronal\n",
    "\n",
    "Ahora descargemos los datos y generemos una red tal cual lo vimos en el notebook anterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                transforms.Normalize([0.5],[0.5])\n",
    "                             ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('logits', nn.Linear(hidden_sizes[1], output_size))]))\n",
    "\n",
    "# NOTA solo calcularemos los logits y definiremos la perdida a partir de ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que definiremos será la función de pérdida (loss) que es nombrada en pytorch como **criterion**. En este ejemplo estamos utilizando softmax, asi que definimos el criterio como *criterion = nn.CrossEntropyLoss()*. Más tarde, en el entrenamiento, veremos que *loss = criterion(output, targets)* calcula la pérdida.\n",
    "\n",
    "Lo segundo que definiremos será el optimizador, para este ejemplo usaremos SGD (stochastic gradient descent). Simplemente llamamos a *torch.optim.SGD* y le pasamos los parámetros de la red y el lerning rate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de relizar el entrenamiento completo haremos un paso del aprendizaje. Este paso se compone de las siguientes tareas:\n",
    "\n",
    "1. Realizar un pase frontal de la red\n",
    "2. Utilizar los logits para calcular la pérdida\n",
    "3. Realizar la retropropagación para calcular los gradientes.\n",
    "4. Actualizar los pesos usando el optimizador.\n",
    "\n",
    "Veamos el ejemplo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0320,  0.0320, -0.0152,  ..., -0.0146, -0.0243,  0.0337],\n",
      "        [-0.0054,  0.0152, -0.0116,  ...,  0.0312, -0.0337, -0.0340],\n",
      "        [ 0.0107,  0.0110, -0.0164,  ..., -0.0074,  0.0237,  0.0246],\n",
      "        ...,\n",
      "        [-0.0340, -0.0299, -0.0295,  ...,  0.0284,  0.0231, -0.0325],\n",
      "        [-0.0001, -0.0007,  0.0217,  ..., -0.0302,  0.0290,  0.0221],\n",
      "        [ 0.0140, -0.0025,  0.0348,  ..., -0.0214,  0.0345, -0.0017]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model.fc1.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient - tensor([[ 0.0005,  0.0005,  0.0005,  ...,  0.0005,  0.0005,  0.0005],\n",
      "        [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "        [ 0.0011,  0.0011,  0.0011,  ...,  0.0011,  0.0011,  0.0011],\n",
      "        ...,\n",
      "        [-0.0027, -0.0027, -0.0027,  ..., -0.0027, -0.0027, -0.0027],\n",
      "        [-0.0008, -0.0008, -0.0008,  ..., -0.0008, -0.0008, -0.0008],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "images, labels = iter(trainloader).next()\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Limpiar los gradientes por que se acumulan\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Pase hacia adelante\n",
    "output = model.forward(images)\n",
    "# Perdida\n",
    "loss = criterion(output, labels)\n",
    "# Pase de reversa\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "# Actualiza los pesos de acuerdo a un paso del optimizador\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 0.0320,  0.0320, -0.0152,  ..., -0.0146, -0.0243,  0.0337],\n",
      "        [-0.0054,  0.0152, -0.0116,  ...,  0.0312, -0.0336, -0.0339],\n",
      "        [ 0.0107,  0.0110, -0.0164,  ..., -0.0074,  0.0237,  0.0246],\n",
      "        ...,\n",
      "        [-0.0340, -0.0299, -0.0294,  ...,  0.0285,  0.0231, -0.0325],\n",
      "        [-0.0001, -0.0007,  0.0217,  ..., -0.0302,  0.0290,  0.0221],\n",
      "        [ 0.0140, -0.0025,  0.0348,  ..., -0.0214,  0.0345, -0.0017]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights - ', model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento por épocas\n",
    "\n",
    "Ahora si, entrenemos la red por varias épocas. Para ello programaremos el algoritmo de gradiente descendente que de forma general funciona con los siguientes pasos:\n",
    "\n",
    "- Para un número de *épocas*:\n",
    "    - Para cada *lote* en el conjunto de datos:\n",
    "        - Salida = Red predice usando el *lote*\n",
    "        - Calcular *pérdida* a partir de la *salida* y de las *etiquetas* reales\n",
    "        - Error = Retropropagación a partir de la *pérdida*\n",
    "        - Actualizar pesos\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuración del optimizador\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5...  Loss: 2.2961\n",
      "Epoch: 1/5...  Loss: 2.2736\n",
      "Epoch: 1/5...  Loss: 2.2552\n",
      "Epoch: 1/5...  Loss: 2.2343\n",
      "Epoch: 1/5...  Loss: 2.2112\n",
      "Epoch: 1/5...  Loss: 2.1819\n",
      "Epoch: 1/5...  Loss: 2.1581\n",
      "Epoch: 1/5...  Loss: 2.1181\n",
      "Epoch: 1/5...  Loss: 2.0934\n",
      "Epoch: 1/5...  Loss: 2.0538\n",
      "Epoch: 1/5...  Loss: 2.0038\n",
      "Epoch: 1/5...  Loss: 1.9668\n",
      "Epoch: 1/5...  Loss: 1.9088\n",
      "Epoch: 1/5...  Loss: 1.8657\n",
      "Epoch: 1/5...  Loss: 1.8113\n",
      "Epoch: 1/5...  Loss: 1.7516\n",
      "Epoch: 1/5...  Loss: 1.6943\n",
      "Epoch: 1/5...  Loss: 1.6202\n",
      "Epoch: 1/5...  Loss: 1.5527\n",
      "Epoch: 1/5...  Loss: 1.4948\n",
      "Epoch: 1/5...  Loss: 1.4182\n",
      "Epoch: 1/5...  Loss: 1.3578\n",
      "Epoch: 1/5...  Loss: 1.3253\n",
      "Epoch: 2/5...  Loss: 0.6951\n",
      "Epoch: 2/5...  Loss: 1.1950\n",
      "Epoch: 2/5...  Loss: 1.1275\n",
      "Epoch: 2/5...  Loss: 1.0853\n",
      "Epoch: 2/5...  Loss: 1.0241\n",
      "Epoch: 2/5...  Loss: 0.9971\n",
      "Epoch: 2/5...  Loss: 0.9750\n",
      "Epoch: 2/5...  Loss: 0.9203\n",
      "Epoch: 2/5...  Loss: 0.9001\n",
      "Epoch: 2/5...  Loss: 0.8471\n",
      "Epoch: 2/5...  Loss: 0.8274\n",
      "Epoch: 2/5...  Loss: 0.7912\n",
      "Epoch: 2/5...  Loss: 0.7567\n",
      "Epoch: 2/5...  Loss: 0.7391\n",
      "Epoch: 2/5...  Loss: 0.7176\n",
      "Epoch: 2/5...  Loss: 0.7120\n",
      "Epoch: 2/5...  Loss: 0.6719\n",
      "Epoch: 2/5...  Loss: 0.6704\n",
      "Epoch: 2/5...  Loss: 0.6466\n",
      "Epoch: 2/5...  Loss: 0.6577\n",
      "Epoch: 2/5...  Loss: 0.6302\n",
      "Epoch: 2/5...  Loss: 0.6345\n",
      "Epoch: 2/5...  Loss: 0.6273\n",
      "Epoch: 3/5...  Loss: 0.0580\n",
      "Epoch: 3/5...  Loss: 0.6013\n",
      "Epoch: 3/5...  Loss: 0.5881\n",
      "Epoch: 3/5...  Loss: 0.5662\n",
      "Epoch: 3/5...  Loss: 0.5659\n",
      "Epoch: 3/5...  Loss: 0.5532\n",
      "Epoch: 3/5...  Loss: 0.5173\n",
      "Epoch: 3/5...  Loss: 0.5519\n",
      "Epoch: 3/5...  Loss: 0.5325\n",
      "Epoch: 3/5...  Loss: 0.5241\n",
      "Epoch: 3/5...  Loss: 0.5213\n",
      "Epoch: 3/5...  Loss: 0.5099\n",
      "Epoch: 3/5...  Loss: 0.5002\n",
      "Epoch: 3/5...  Loss: 0.5044\n",
      "Epoch: 3/5...  Loss: 0.4929\n",
      "Epoch: 3/5...  Loss: 0.4768\n",
      "Epoch: 3/5...  Loss: 0.5065\n",
      "Epoch: 3/5...  Loss: 0.4837\n",
      "Epoch: 3/5...  Loss: 0.4560\n",
      "Epoch: 3/5...  Loss: 0.4626\n",
      "Epoch: 3/5...  Loss: 0.4663\n",
      "Epoch: 3/5...  Loss: 0.4771\n",
      "Epoch: 3/5...  Loss: 0.4734\n",
      "Epoch: 3/5...  Loss: 0.4604\n",
      "Epoch: 4/5...  Loss: 0.2853\n",
      "Epoch: 4/5...  Loss: 0.4411\n",
      "Epoch: 4/5...  Loss: 0.4220\n",
      "Epoch: 4/5...  Loss: 0.4662\n",
      "Epoch: 4/5...  Loss: 0.4588\n",
      "Epoch: 4/5...  Loss: 0.4600\n",
      "Epoch: 4/5...  Loss: 0.4477\n",
      "Epoch: 4/5...  Loss: 0.4174\n",
      "Epoch: 4/5...  Loss: 0.4283\n",
      "Epoch: 4/5...  Loss: 0.4004\n",
      "Epoch: 4/5...  Loss: 0.4604\n",
      "Epoch: 4/5...  Loss: 0.4216\n",
      "Epoch: 4/5...  Loss: 0.4187\n",
      "Epoch: 4/5...  Loss: 0.4609\n",
      "Epoch: 4/5...  Loss: 0.4143\n",
      "Epoch: 4/5...  Loss: 0.4235\n",
      "Epoch: 4/5...  Loss: 0.4213\n",
      "Epoch: 4/5...  Loss: 0.4078\n",
      "Epoch: 4/5...  Loss: 0.4341\n",
      "Epoch: 4/5...  Loss: 0.4048\n",
      "Epoch: 4/5...  Loss: 0.3985\n",
      "Epoch: 4/5...  Loss: 0.3816\n",
      "Epoch: 4/5...  Loss: 0.3866\n",
      "Epoch: 5/5...  Loss: 0.0728\n",
      "Epoch: 5/5...  Loss: 0.4147\n",
      "Epoch: 5/5...  Loss: 0.4077\n",
      "Epoch: 5/5...  Loss: 0.4147\n",
      "Epoch: 5/5...  Loss: 0.3976\n",
      "Epoch: 5/5...  Loss: 0.3728\n",
      "Epoch: 5/5...  Loss: 0.3989\n",
      "Epoch: 5/5...  Loss: 0.3644\n",
      "Epoch: 5/5...  Loss: 0.3876\n",
      "Epoch: 5/5...  Loss: 0.4099\n",
      "Epoch: 5/5...  Loss: 0.3700\n",
      "Epoch: 5/5...  Loss: 0.3601\n",
      "Epoch: 5/5...  Loss: 0.3677\n",
      "Epoch: 5/5...  Loss: 0.3872\n",
      "Epoch: 5/5...  Loss: 0.3665\n",
      "Epoch: 5/5...  Loss: 0.4056\n",
      "Epoch: 5/5...  Loss: 0.3790\n",
      "Epoch: 5/5...  Loss: 0.3465\n",
      "Epoch: 5/5...  Loss: 0.3896\n",
      "Epoch: 5/5...  Loss: 0.3704\n",
      "Epoch: 5/5...  Loss: 0.3918\n",
      "Epoch: 5/5...  Loss: 0.3618\n",
      "Epoch: 5/5...  Loss: 0.3950\n",
      "Epoch: 5/5...  Loss: 0.3928\n"
     ]
    }
   ],
   "source": [
    "# hiperparámetro: número de épocas\n",
    "epochs = 5\n",
    "print_every = 40\n",
    "steps = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    # en cada iteración del for cargamos un batch\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Aplanar las imagenes de MNIST\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        # Reiniciar el gradiente\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pase frontal de la red\n",
    "        output = model.forward(images)\n",
    "        \n",
    "        # Estimar la perdida\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backprogation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Actualizamos los pesos\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Guardamos la perdida para control del entrenamiento\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # imprimimos cada 40 lotes\n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "            \n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, veamos que tan bien está clasificando la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 235,
       "width": 424
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "[1] Udacity Computer vision Nanodegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
