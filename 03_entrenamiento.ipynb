{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de una RN\n",
    "\n",
    "Al entrenar una red buscamos que ésta tenga un comportamiendo presumiblemente observable en un conjunto de datos. Es decír a partir del conjunto de ejemplos esperamos que la red aprenda una función $f$ tal que la salida de la red imite el patrón en los datos. En nuestro caso, al utilizar el conjunto de entrenamiento MNIST esperamos que la función aprendida provea como salida el número al cual corresponde la imagen de entrada.\n",
    "\n",
    "<img src=\"archivos/function_approx.png\">\n",
    "Imagen tomada de Udacity[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradientes y Autograd\n",
    "\n",
    "Pytorch provee el módulo *autograd* para calcular los gradientes, y si, !nos evita estar calculando las derivadas! Esto lo realiza a partir de mantener en la vista todas las operaciones que se hacen a los tensores.\n",
    "\n",
    "Si deseas asegurarte que autograd siga a un tensor especificamos *requires_grad*. Esto se puede hacer en la creación o en cualquier momento. \n",
    "\n",
    "Veamos el siguiente código de ejemplo:\n",
    "\n",
    "```python\n",
    "# especificamos que la variable x es seguida por autograd\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    "# si en algún momento desamos que temporalmente se deje de seguir el tensor usamos\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    "# establecemos de nuevo el seguimiento\n",
    ">>> y.requires_grad\n",
    "```\n",
    "\n",
    "Si queremos eliminar autograd de todos los tensores usamos `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "\n",
    "Ahora bien, para calcular los gradientes simplemente usamos el método *backward()*. Por ejemplo para un tensor *x* hacemos *z.backward()*\n",
    "\n",
    "Veamos el uso del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos paquetes\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1440,  1.3263],\n",
      "        [ 0.5819, -0.1294]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# especificamos que el tensor x es seguido por autograd \n",
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0207, 1.7591],\n",
      "        [0.3386, 0.0168]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# generamos un nuevo tensor a partir de x\n",
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x7fa1e8116518>\n"
     ]
    }
   ],
   "source": [
    "## con grad_fn observamos la operación que generó y, es decir una operación potencia (pow)\n",
    "print(y.grad_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5338, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# De esta forma es posible saber las operaciónes que generan cada tensor, y por tanto, es posible calcular el gradiente.\n",
    "\n",
    "# Hagamos ahora una operación de media\n",
    "\n",
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# hasta este momento los gradientes son cero\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular los gradientes es necesario llamar al método *.backward* sobre la variable. Supongamos sobre *z*. Esto calcula el gradiente de z con respecto de x.\n",
    "\n",
    "El gradiente analítico de las operaciónes que hicimos es:\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$\n",
    "\n",
    "Ahora comprobemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0720,  0.6632],\n",
      "        [ 0.2910, -0.0647]])\n",
      "tensor([[ 0.0720,  0.6632],\n",
      "        [ 0.2910, -0.0647]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y Red Neuronal\n",
    "\n",
    "Ahora descargemos los datos y generemos una red tal cual lo vimos en el notebook anterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                             ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('logits', nn.Linear(hidden_sizes[1], output_size))]))\n",
    "\n",
    "# NOTA solo calcularemos los logits y definiremos la perdida a partir de ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la RN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que definiremos será la función de pérdida (loss) que es nombrada en pytorch como **criterion**. En este ejemplo estamos utilizando softmax, asi que definimos el criterio como *criterion = nn.CrossEntropyLoss()*. Más tarde, en el entrenamiento, veremos que *loss = criterion(output, targets)* calcula la pérdida.\n",
    "\n",
    "Lo segundo que definiremos será el optimizador, para este ejemplo usaremos SGD (stochastic gradient descent). Simplemente llamamos a *torch.optim.SGD* y le pasamos los parámetros de la red y el lerning rate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de relizar el entrenamiento completo haremos un paso del aprendizaje. Este paso se compone de las siguientes tareas:\n",
    "\n",
    "1. Realizar un pase frontal de la red\n",
    "2. Utilizar los logits para calcular la pérdida\n",
    "3. Realizar un pase en reversa para calcular los gradientes.\n",
    "4. Actualizar los pesos usando el optimizador.\n",
    "\n",
    "Veamos el ejemplo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0225, -0.0183, -0.0109,  ..., -0.0325,  0.0132, -0.0265],\n",
      "        [ 0.0293,  0.0057, -0.0163,  ..., -0.0111, -0.0273,  0.0343],\n",
      "        [ 0.0249, -0.0158,  0.0322,  ..., -0.0344,  0.0321, -0.0269],\n",
      "        ...,\n",
      "        [-0.0228,  0.0120, -0.0271,  ..., -0.0050,  0.0291,  0.0337],\n",
      "        [-0.0322,  0.0170, -0.0128,  ..., -0.0277,  0.0251,  0.0003],\n",
      "        [-0.0151,  0.0169,  0.0311,  ...,  0.0344, -0.0242, -0.0240]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model.fc1.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient - tensor([[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0017,  0.0017,  0.0017],\n",
      "        [ 0.0001,  0.0001,  0.0001,  ...,  0.0001,  0.0001,  0.0001],\n",
      "        ...,\n",
      "        [-0.0004, -0.0004, -0.0004,  ..., -0.0004, -0.0004, -0.0004],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
      "        [ 0.0042,  0.0042,  0.0042,  ...,  0.0042,  0.0042,  0.0042]])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Limpiar los gradientes por que se acumulan\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Pase hacia adelante\n",
    "output = model.forward(images)\n",
    "# Perdida\n",
    "loss = criterion(output, labels)\n",
    "# Pase de reversa\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "# Actualiza los pesos de acuerdo a un paso del optimizador\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 0.0225, -0.0183, -0.0108,  ..., -0.0325,  0.0132, -0.0265],\n",
      "        [ 0.0293,  0.0057, -0.0163,  ..., -0.0112, -0.0273,  0.0343],\n",
      "        [ 0.0249, -0.0158,  0.0322,  ..., -0.0344,  0.0321, -0.0269],\n",
      "        ...,\n",
      "        [-0.0228,  0.0120, -0.0271,  ..., -0.0050,  0.0291,  0.0337],\n",
      "        [-0.0322,  0.0170, -0.0128,  ..., -0.0277,  0.0251,  0.0003],\n",
      "        [-0.0151,  0.0168,  0.0311,  ...,  0.0344, -0.0243, -0.0240]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights - ', model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento por épocas\n",
    "\n",
    "Ahora si, entrenemos la red por varias épocas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuración del optimizador\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3...  Loss: 2.3043\n",
      "Epoch: 1/3...  Loss: 2.2875\n",
      "Epoch: 1/3...  Loss: 2.2668\n",
      "Epoch: 1/3...  Loss: 2.2503\n",
      "Epoch: 1/3...  Loss: 2.2288\n",
      "Epoch: 1/3...  Loss: 2.2081\n",
      "Epoch: 1/3...  Loss: 2.1830\n",
      "Epoch: 1/3...  Loss: 2.1528\n",
      "Epoch: 1/3...  Loss: 2.1213\n",
      "Epoch: 1/3...  Loss: 2.0905\n",
      "Epoch: 1/3...  Loss: 2.0570\n",
      "Epoch: 1/3...  Loss: 2.0129\n",
      "Epoch: 1/3...  Loss: 1.9700\n",
      "Epoch: 1/3...  Loss: 1.9250\n",
      "Epoch: 1/3...  Loss: 1.8609\n",
      "Epoch: 1/3...  Loss: 1.8056\n",
      "Epoch: 1/3...  Loss: 1.7367\n",
      "Epoch: 1/3...  Loss: 1.6843\n",
      "Epoch: 1/3...  Loss: 1.6139\n",
      "Epoch: 1/3...  Loss: 1.5398\n",
      "Epoch: 1/3...  Loss: 1.4718\n",
      "Epoch: 1/3...  Loss: 1.3979\n",
      "Epoch: 1/3...  Loss: 1.3337\n",
      "Epoch: 2/3...  Loss: 0.6942\n",
      "Epoch: 2/3...  Loss: 1.2206\n",
      "Epoch: 2/3...  Loss: 1.1915\n",
      "Epoch: 2/3...  Loss: 1.0995\n",
      "Epoch: 2/3...  Loss: 1.0551\n",
      "Epoch: 2/3...  Loss: 1.0209\n",
      "Epoch: 2/3...  Loss: 0.9777\n",
      "Epoch: 2/3...  Loss: 0.9203\n",
      "Epoch: 2/3...  Loss: 0.8960\n",
      "Epoch: 2/3...  Loss: 0.8792\n",
      "Epoch: 2/3...  Loss: 0.8299\n",
      "Epoch: 2/3...  Loss: 0.8160\n",
      "Epoch: 2/3...  Loss: 0.8148\n",
      "Epoch: 2/3...  Loss: 0.7805\n",
      "Epoch: 2/3...  Loss: 0.7506\n",
      "Epoch: 2/3...  Loss: 0.7311\n",
      "Epoch: 2/3...  Loss: 0.7094\n",
      "Epoch: 2/3...  Loss: 0.7156\n",
      "Epoch: 2/3...  Loss: 0.6835\n",
      "Epoch: 2/3...  Loss: 0.6739\n",
      "Epoch: 2/3...  Loss: 0.6743\n",
      "Epoch: 2/3...  Loss: 0.6399\n",
      "Epoch: 2/3...  Loss: 0.6470\n",
      "Epoch: 3/3...  Loss: 0.0561\n",
      "Epoch: 3/3...  Loss: 0.5951\n",
      "Epoch: 3/3...  Loss: 0.6150\n",
      "Epoch: 3/3...  Loss: 0.6200\n",
      "Epoch: 3/3...  Loss: 0.5855\n",
      "Epoch: 3/3...  Loss: 0.5965\n",
      "Epoch: 3/3...  Loss: 0.5806\n",
      "Epoch: 3/3...  Loss: 0.5886\n",
      "Epoch: 3/3...  Loss: 0.5510\n",
      "Epoch: 3/3...  Loss: 0.5459\n",
      "Epoch: 3/3...  Loss: 0.5653\n",
      "Epoch: 3/3...  Loss: 0.5493\n",
      "Epoch: 3/3...  Loss: 0.5591\n",
      "Epoch: 3/3...  Loss: 0.5228\n",
      "Epoch: 3/3...  Loss: 0.5087\n",
      "Epoch: 3/3...  Loss: 0.5049\n",
      "Epoch: 3/3...  Loss: 0.4829\n",
      "Epoch: 3/3...  Loss: 0.4925\n",
      "Epoch: 3/3...  Loss: 0.4824\n",
      "Epoch: 3/3...  Loss: 0.4926\n",
      "Epoch: 3/3...  Loss: 0.4901\n",
      "Epoch: 3/3...  Loss: 0.4975\n",
      "Epoch: 3/3...  Loss: 0.4816\n",
      "Epoch: 3/3...  Loss: 0.4763\n"
     ]
    }
   ],
   "source": [
    "# hiperparámetro: número de épocas\n",
    "epochs = 3\n",
    "print_every = 40\n",
    "steps = 0\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    # en cada iteración del for cargamos un batch\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward and backward passes\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        # actualizamos los pesos\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        # imprimimos cada 40 batches\n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "            \n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, veamos que tan bien está clasificando la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helper' has no attribute 'view_classify'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3345db404db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Output of the network are logits, need to take softmax for probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'helper' has no attribute 'view_classify'"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
